<html>
	<head>
		<title>Primitive Intelligence</title>		
	    <style type="text/css">
<!--
.style15 {	font-family: Verdana, Arial, Helvetica, sans-serif;
	font-size: 24px;
	font-weight: bold;
	color: #FFFFFF;
}
p.header {			font-size: small;
}
-->
        </style>
</head>
	<script type="text/javascript" src="./scripts/jquery-1.11.3.min.js"></script>
	<script type="text/javascript" src="./scripts/pitch_detection.js"></script>
	<link rel="stylesheet" type="text/css" href="./css/main.css"></link>
	<body>
		 <script type="text/javascript">
      $(document).ready(function(){
        PitchDetection.start();        
      });
    </script>
		 <span class="style15">Primitive Intelligence</span>
    <hr noshade="noshade" color: #333333 />
    <p>Primitive Intelligence is an intelligence powered  by Web Audio API and WebMIDI. It understands what it is spoken in a primitive  way and responds in a primitive way. As opposed to the high technology AIs and  UIs, <em>Primitive Intelligence</em> does not  understand natural language, defined commands or even language at all. Instead  it understands the sonic properties of the spoken language like the attack,  loudness, roughness etc&hellip; And it responds in a non-language way, not equipped  with the symbolic sounds that we call language, <em>Primitive Intelligence</em> responds to the audience/user/player with  tonal sounds using timbre, frequency, loudness and time to create a response. It lacks the meaning of the symbols but can understand the emotions. Primitive Intelligence is the antithesis for the modern AIs such as Siri, Cortana, OK Google that we use daily. These AIs understand our language and respond in our language. They have fine tuned intonations that show how they feel as they deliver the information gathered from the cloud. Primitive Intelligence tries to understand the emotion in the human voice - free from symbols of the language- and responds you with your emotions like sonic mirror to the soul.</p>
    <p>Primitive Intelligence is currently in progress.</p>
    <p>Current Version (0.01): The system can respond to a talking by mimicking the fundamental frequency of the speech.</p>
    <p>Next Steps: </p>
    <p>- The intelligence learns how to listen to others (v0.02):  push-to-talk to start the system, after release of the button the system responds.</p>
    <p>- The intelligence can understand more frequencies from the speech (v0.03): The system can detect the harmonics thus can detect dissonance and consonance.</p>
    <p>- The intelligence sharpens its existing skills (v0.04): General polish and improvements.</p>
    <p>- The intelligence can respond using a larger vocabulary (v0.1): Web MIDI api support added.</p>
    <p>- The intelligence can understand more subtleties of human speech (v0.11): Analysis of the envolopes and matching them to emotions using Emotional Reaction Model (ERM) as proposed by Juslin and Vastfjal. (&ldquo;Emotional  responses to music: The need to consider underlying mechanisms.&rdquo; <em>Behavioral and Brain Sciences </em>31,  (2008): 559-621.)</p>
    <p>- The intelligence can use its new understanding to respond to people (v0.12): Using the analysis results and ERM to create responses</p>
    <p>- The intelligence sharpens its existing skills (v0.13): General polish and improvements.</p>
    <p>- The intelligence can control its facial expression while listening  (v0.2): A visualizer will be added to system that matches the aesthetics of the audio.</p>
    <p>- The intelligence can control its facial expression while talking (v0.21): A visualizer will be added to system that matches the aesthetics of the audio.</p>
    <p>- The intelligence sharpens its existing skills (v0.22): General polish and improvements.</p>
    <p>&nbsp;</p>
    <p class="header"></p>
</body>
</html>
