<html>
	<head>
		<title>Primitive Intelligence</title>		
	    <style type="text/css">
<!--
.style15 {	font-family: Verdana, Arial, Helvetica, sans-serif;
	font-size: 24px;
	font-weight: bold;
	color: #FFFFFF;
}
p.header {			font-size: small;
}
-->
        </style>
</head>
	<script type="text/javascript" src="./scripts/jquery-1.11.3.min.js"></script>
	<script type="text/javascript" src="./scripts/pitch_detection.js"></script>
	<link rel="stylesheet" type="text/css" href="./css/main.css"></link>
	<body>
		 <script type="text/javascript">
      $(document).ready(function(){
        PitchDetection.start();        
      });
    </script>
		 <span class="style15">Primitive Intelligence</span>
    <hr noshade="noshade" color: #333333 />
    <p><em>Primitive Intelligence</em> is an intelligent system powered by Web Audio API and WebMIDI. It understands  what it is spoken in a primitive way and responds to us in a primitive,  emotional and simple way. As opposed to the cutting edge AIs and interfaces  like Siri, Cortana, OK Google, <em>Primitive  Intelligence</em> does not understand natural language, defined commands or even  language at all. It does not respond using fine-tuned intonations to deliver  the information gathered from the cloud. Instead, as an antithesis, it only  tries to understand the sonic properties of the spoken language like the  attack, loudness, roughness etc&hellip; And responds in a non-language, musical,  psychoacoustical way: without the symbolic sounds that we call language but  with tonal sounds, timbre, frequency, consonance and rhythm. Ignoring the  symbolic data, like an infant, <em>Primitive  Intelligence</em> tries to understand the emotion in the human voice - free from  symbols of the language - and responds you with your emotions like sonic mirror  to the soul. The aimed interactive experience brings questions about human  sound that we sometimes ignore in this age of information.</p>
    <p>Primitive Intelligence is currently in progress to create the vision outlined above.</p>
    <p>Current Version (0.01): The system can respond to a talking by mimicking the fundamental frequency of the speech.</p>
    <p>Next Steps: </p>
    <p>- The intelligence learns how to listen to others (v0.02):  push-to-talk to start the system, after release of the button the system responds.</p>
    <p>- The intelligence can understand more frequencies from the speech (v0.03): The system can detect the harmonics thus can detect dissonance and consonance.</p>
    <p>- The intelligence sharpens its existing skills (v0.04): General polish and improvements.</p>
    <p>- The intelligence can respond using a larger vocabulary (v0.1): Web MIDI api support added.</p>
    <p>- The intelligence can understand more subtleties of human speech (v0.11): Analysis of the envolopes and matching them to emotions using Emotional Reaction Model (ERM) as proposed by Juslin and Vastfjal. (&ldquo;Emotional  responses to music: The need to consider underlying mechanisms.&rdquo; <em>Behavioral and Brain Sciences </em>31,  (2008): 559-621.)</p>
    <p>- The intelligence can use its new understanding to respond to people (v0.12): Using the analysis results and ERM to create responses</p>
    <p>- The intelligence sharpens its existing skills (v0.13): General polish and improvements.</p>
    <p>- The intelligence can control its facial expression while listening  (v0.2): A visualizer will be added to system that matches the aesthetics of the audio.</p>
    <p>- The intelligence can control its facial expression while talking (v0.21): A visualizer will be added to system that matches the aesthetics of the audio.</p>
    <p>- The intelligence sharpens its existing skills (v0.22): General polish and improvements.</p>
    <p>&nbsp;</p>
    <p class="header"></p>
</body>
</html>
